{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8af63472",
   "metadata": {},
   "source": [
    "# An example of using BRepNet embeddings for face similarity matching\n",
    "This example notebook shows how the embeddings generated by the BRepNet model can be used for other tasks like selecitng similar faces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3266e0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "if os.path.isfile('../models/brepnet.py'):\n",
    "    # We are in the notebooks directory.  Change to the root\n",
    "    os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a734c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# This code allows you to evaluate a pre-trained model for all step files in a folder\n",
    "from eval.evaluate_folder import evaluate_folder\n",
    "\n",
    "# This viewer allows you to visualize the results\n",
    "from visualization.jupyter_segmentation_viewer import JupyterSegmentationViewer\n",
    "\n",
    "from visualization.save_images_of_similar_solids import SimilarSolidImageSaver\n",
    "from occwl.solid import Solid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5445545",
   "metadata": {},
   "source": [
    "Now we would like to evaluate the model for some example files.  We can do this using `eval/evaluate_folder.py`.  We need to supply the script with the path to the step files to evaluate, the feature standadization and the pretrained model to use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f91cef02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed pipeline/extract_feature_data_from_step.py\n",
      "Using labels from example_files/step_examples/temp_working\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lambouj/anaconda3/envs/brepnet/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py:1585: UserWarning: GPU available but not used. Set the gpus flag in your trainer `Trainer(gpus=1)` or script `--gpus=1`.\n",
      "  \"GPU available but not used. Set the gpus flag in your trainer `Trainer(gpus=1)` or script `--gpus=1`.\"\n",
      "/home/lambouj/anaconda3/envs/brepnet/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 36 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9f8b40a89a24a3fbeb04862392b830a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test/Chamfer_iou': 0.8418079018592834,\n",
      " 'test/CutEnd_iou': 0.7290322780609131,\n",
      " 'test/CutSide_iou': 0.8102856874465942,\n",
      " 'test/ExtrudeEnd_iou': 0.7076271176338196,\n",
      " 'test/ExtrudeSide_iou': 0.8099502325057983,\n",
      " 'test/Fillet_iou': 0.9281437397003174,\n",
      " 'test/RevolveEnd_iou': 0.8999999761581421,\n",
      " 'test/RevolveSide_iou': 0.7386363744735718,\n",
      " 'test/accuracy': 0.8996027708053589,\n",
      " 'test/mean_iou': 0.8081854581832886}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Here is the path to some example step files for us to convert\n",
    "step_folder = Path(\"./example_files/step_examples\")\n",
    "\n",
    "# You may want to run this on the entire extended STEP dataset which you can doanload from \n",
    "# https://fusion-360-gallery-dataset.s3.us-west-2.amazonaws.com/segmentation/s2.0.0/s2.0.0_extended_step.zip\n",
    "# step_folder = Path(\"/path/to/s2.0.0_extended_step/breps/step/\")\n",
    "\n",
    "# We will also need to know the feature standardization for the dataset used to train the model\n",
    "# This is found in the dataset file created by pipeline/build_dataset_file.py or pipeline/quickstart.py\n",
    "feature_standardization = Path(\"./example_files/feature_standardization/s2.0.0_step_all_features.json\")\n",
    "\n",
    "# Here is the path to a pretrained model\n",
    "pretrained_model = Path(\"./example_files/pretrained_models/pretrained_s2.0.0_extended_step_uv_net_features_0816_183419.ckpt\")\n",
    "\n",
    "# Now we can evaluate the model on these step files.\n",
    "# Depending on your system you may see pytorch lightning warning you  \n",
    "# GPUs are unused and more worker threads could be used in the dataloader.\n",
    "# The default options here are intended to work on a minimal system.\n",
    "evaluate_folder(step_folder, feature_standardization, model=pretrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c51c4437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We found 25 example files\n"
     ]
    }
   ],
   "source": [
    "step_file_stems = [ f.stem for f in step_folder.glob(\"*.stp\")]\n",
    "print(f\"We found {len(step_file_stems)} example files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95b4156",
   "metadata": {},
   "source": [
    "Choose the model you would like to use as the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18722acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_index = 17"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1055a01e",
   "metadata": {},
   "source": [
    "Now you will need to select some faces on the solid to continue.  \n",
    "Double click on each face to select it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13fd9a92",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viewing example 21242_6c2af7c2_7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb42859f955b4f5d9fa2ce6af7898a0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(HBox(children=(Checkbox(value=True, description='Axes', layout=Layout(height='au…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file_stem = step_file_stems[query_index]\n",
    "print(f\"Viewing example {file_stem}\")\n",
    "viewer = JupyterSegmentationViewer(file_stem, step_folder, seg_folder=step_folder)\n",
    "viewer.view_solid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1893d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(viewer.selection_list) > 0, \"Please select some faces on the solid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a98a6c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the embeddings for the query solid\n",
    "embeddings_folder = step_folder / \"temp_working/embeddings\"\n",
    "embeddings_pathname = embeddings_folder / (file_stem + \".embeddings\")\n",
    "embeddings = np.loadtxt(embeddings_pathname)\n",
    "assert embeddings.shape[0] == len(viewer.entity_mapper.face_map), \"Embedding size doesn't match solid\"\n",
    "selected_face_embeddings = embeddings[viewer.selection_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a563b97",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Loop over all solids and find the distance from each face of each solid\n",
    "# to each query face\n",
    "all_embeddings = list(embeddings_folder.glob(\"*.embeddings\"))\n",
    "min_dists_for_each_face = []\n",
    "sum_min_dists_for_each_solid = []\n",
    "for embedding_file in all_embeddings:\n",
    "    target_file_stem = embedding_file.stem\n",
    "    \n",
    "    # Load the embeddings for the target file\n",
    "    embeddings = np.loadtxt(embedding_file)\n",
    "    if len(embeddings.shape) == 1:\n",
    "        embeddings = np.expand_dims(embeddings, axis=0)\n",
    "        \n",
    "    min_dists = []\n",
    "    min_dists_for_each_face_in_this_solid = []\n",
    "    for face_embedding in selected_face_embeddings:\n",
    "        face_to_embeddings = embeddings-face_embedding\n",
    "        dist = np.linalg.norm(face_to_embeddings, axis=1)\n",
    "        \n",
    "        # Here we compute the distance from a given query face\n",
    "        # to each face in the solid\n",
    "        min_dists_for_each_face_in_this_solid.append(dist)\n",
    "        \n",
    "        # Then is metric is the distance from that given query face to\n",
    "        # the most similar face in the solid\n",
    "        min_dists.append(np.min(dist))\n",
    "    \n",
    "    # Now we need some way to say which solids is the best match over all\n",
    "    # We do this by summing the minimum distances to all the faces \n",
    "    sum_dists = np.sum(np.stack(min_dists), axis=0)\n",
    "    sum_min_dists_for_each_solid.append(sum_dists)\n",
    "    \n",
    "    # For display we also keep track of the min distance to each face\n",
    "    min_dists_for_each_face_in_this_solid = np.min(np.stack(min_dists_for_each_face_in_this_solid), axis=0)\n",
    "    min_dists_for_each_face.append(min_dists_for_each_face_in_this_solid)\n",
    "    \n",
    "sum_min_dists_for_each_solid = np.array(sum_min_dists_for_each_solid)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c64b822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interval [0.0, 18.090275079197]\n"
     ]
    }
   ],
   "source": [
    "# Now we will search for the top k best matches\n",
    "k = 5\n",
    "indices_of_smallest = np.argpartition(sum_min_dists_for_each_solid, kth=range(k))[:k]\n",
    "\n",
    "# We want to show how close each face in each solid is\n",
    "# To do this we want some kind of range which is computed here\n",
    "all_dists_top_k = []\n",
    "for index in indices_of_smallest:\n",
    "    all_dists_top_k.append(min_dists_for_each_face[index])\n",
    "all_dists_top_k = np.concatenate(all_dists_top_k)\n",
    "interval = [all_dists_top_k.min(), all_dists_top_k.max()]\n",
    "print(f\"Interval {interval}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "893187c8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "Close file 21242_6c2af7c2_7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "340dc280c2594ac3b40371d5f8c1d871",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(HBox(children=(Checkbox(value=True, description='Axes', layout=Layout(height='au…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "Close file 56436_2a8fc254_3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0f1deac6c59496da6b77cd96135e3cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(HBox(children=(Checkbox(value=True, description='Axes', layout=Layout(height='au…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "Close file 24051_4852a192_5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3aae6cd4682480ab67457dbf75994ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(HBox(children=(Checkbox(value=True, description='Axes', layout=Layout(height='au…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "Close file 44647_d83249a9_0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5000d0d66da4b4681bd2ca7eb8a63f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(HBox(children=(Checkbox(value=True, description='Axes', layout=Layout(height='au…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "Close file 21492_8bd34fc1_15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0312716185a849048925c23bae453aeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(HBox(children=(Checkbox(value=True, description='Axes', layout=Layout(height='au…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now as a sanity check we always expect to find the query as the best \n",
    "# match.  The other matching faces (red) should have a similar shape\n",
    "for i, index in enumerate(indices_of_smallest):\n",
    "    print(index)\n",
    "    close_file_stem = all_embeddings[index].stem\n",
    "    print(f\"Close file {close_file_stem}\")\n",
    "    close_viewer = JupyterSegmentationViewer(close_file_stem, step_folder)\n",
    "    dists_to_view = min_dists_for_each_face[index]\n",
    "    close_viewer.display_faces_with_heatmap(dists_to_view, interval)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
